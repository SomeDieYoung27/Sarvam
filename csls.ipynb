{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNBeea8kyi+YpqrfM1qEpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeDieYoung27/Sarvam/blob/main/csls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekWPK94cZ2NY"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gensim==4.3.3\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import KeyedVectors\n",
        "import gc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FASTTEXT embedding files\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ],
      "metadata": {
        "id": "7UliMxVAaufL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjusted based on available memory\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 5\n",
        "K_PRECISION = [1, 5]"
      ],
      "metadata": {
        "id": "Yej-bQITv9Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bilingual dictionaries download\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ],
      "metadata": {
        "id": "FHpOqCd7wAU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load embeddings for top N words\n",
        "def load_top_n_embeddings(embedding_file, max_vocab_size):\n",
        "    embeddings = {}\n",
        "    with open(embedding_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        next(f)  # Skip header\n",
        "        for idx, line in enumerate(f):\n",
        "            if idx >= max_vocab_size:\n",
        "                break\n",
        "            parts = line.rstrip().split(' ')\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "tvY0sLrOwFET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decompress and load embeddings for top N words\n",
        "!gunzip -k cc.en.300.vec.gz\n",
        "!gunzip -k cc.hi.300.vec.gz\n",
        "en_embeddings_dict = load_top_n_embeddings('cc.en.300.vec', MAX_VOCAB_SIZE)\n",
        "hi_embeddings_dict = load_top_n_embeddings('cc.hi.300.vec', MAX_VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "tmk0jFMFwIkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KeyedVectors\n",
        "def create_keyed_vectors(embeddings_dict):\n",
        "    kv = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "    kv.add_vectors(list(embeddings_dict.keys()), list(embeddings_dict.values()))\n",
        "    return kv\n",
        "\n",
        "en_embeddings = create_keyed_vectors(en_embeddings_dict)\n",
        "hi_embeddings = create_keyed_vectors(hi_embeddings_dict)"
      ],
      "metadata": {
        "id": "0X2lexz9uPsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load bilingual dictionaries\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                bilingual_dict.append((parts[0], parts[1]))\n",
        "    return bilingual_dict"
      ],
      "metadata": {
        "id": "jHxTcQbE8XVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and Test dictionary\n",
        "train_dict = load_bilingual_lexicon('en-hi.0-5000.txt')\n",
        "test_dict = load_bilingual_lexicon('en-hi.5000-6500.txt')"
      ],
      "metadata": {
        "id": "I6UgzB6P8jdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert embeddings to torch tensors and normalize\n",
        "src_embeddings = torch.from_numpy(en_embeddings.vectors).float()\n",
        "tgt_embeddings = torch.from_numpy(hi_embeddings.vectors).float()\n",
        "\n",
        "src_embeddings = src_embeddings / src_embeddings.norm(2, dim=1, keepdim=True)\n",
        "tgt_embeddings = tgt_embeddings / tgt_embeddings.norm(2, dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "PhhYNeII8siO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move embeddings to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "src_embeddings = src_embeddings.to(device)\n",
        "tgt_embeddings = tgt_embeddings.to(device)"
      ],
      "metadata": {
        "id": "P3mdPvOh8xt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Discriminator with Weight Normalization for stability\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=2048):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "rBvgOhLm81M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize mapping (generator) and discriminator\n",
        "mapping = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n",
        "nn.init.eye_(mapping.weight)  # Initialize as identity matrix\n",
        "discriminator = Discriminator(EMBEDDING_DIM)\n"
      ],
      "metadata": {
        "id": "xl1mCG1x84om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move models to device\n",
        "mapping = mapping.to(device)\n",
        "discriminator = discriminator.to(device)"
      ],
      "metadata": {
        "id": "Bm271Uo8867k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizers with adjusted learning rates\n",
        "lr = 0.1\n",
        "mapping_optimizer = optim.SGD(mapping.parameters(), lr=lr)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "l2zP0OcD8-Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to orthogonalize mapping matrix\n",
        "def orthogonalize(W):\n",
        "    with torch.no_grad():\n",
        "        W.copy_(torch.linalg.qr(W)[0])"
      ],
      "metadata": {
        "id": "WONivcrX9D2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial Training Loop with Orthogonality Constraint\n",
        "for epoch in range(N_EPOCHS):\n",
        "    # Shuffle indices\n",
        "    indices = torch.randperm(src_embeddings.size(0))\n",
        "    num_batches = src_embeddings.size(0) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Get batch indices\n",
        "        batch_indices = indices[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        # Get source and target batch embeddings\n",
        "        src_batch = src_embeddings[batch_indices]\n",
        "        tgt_batch = tgt_embeddings[batch_indices]\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch).detach()\n",
        "        # Ensure mapped embeddings are normalized\n",
        "        src_mapped = src_mapped / src_mapped.norm(2, dim=1, keepdim=True)\n",
        "        # Discriminator outputs\n",
        "        src_preds = discriminator(src_mapped)\n",
        "        tgt_preds = discriminator(tgt_batch)\n",
        "        # Labels\n",
        "        src_labels = torch.zeros(src_batch.size(0), 1).to(device)\n",
        "        tgt_labels = torch.ones(tgt_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        d_loss_src = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        d_loss_tgt = nn.functional.binary_cross_entropy_with_logits(tgt_preds, tgt_labels)\n",
        "        d_loss = d_loss_src + d_loss_tgt\n",
        "        # Backpropagation\n",
        "        d_loss.backward()\n",
        "        # Gradient Clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 5)\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train mapping (generator)\n",
        "        mapping_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch)\n",
        "        # Ensure mapped embeddings are normalized\n",
        "        src_mapped = src_mapped / src_mapped.norm(2, dim=1, keepdim=True)\n",
        "        # Discriminator output\n",
        "        src_preds = discriminator(src_mapped)\n",
        "        # Labels (want discriminator to think mapped embeddings are target)\n",
        "        src_labels = torch.ones(src_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        g_loss = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        # Backpropagation\n",
        "        g_loss.backward()\n",
        "        # Gradient Clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(mapping.parameters(), 5)\n",
        "        mapping_optimizer.step()\n",
        "\n",
        "        # Enforce orthogonality constraint\n",
        "        orthogonalize(mapping.weight.data)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")')\n"
      ],
      "metadata": {
        "id": "2t6Dvprk9GPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the mapping matrix\n",
        "W_adv = mapping.weight.data.cpu().numpy()"
      ],
      "metadata": {
        "id": "Yo5DlyKj9QDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map and normalize English embeddings\n",
        "en_embeddings_mapped = en_embeddings.vectors @ W_adv.T\n",
        "en_embeddings_mapped = en_embeddings_mapped / np.linalg.norm(en_embeddings_mapped, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "DHM67hOv9bre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KeyedVectors instance for mapped embeddings\n",
        "en_mapped = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "en_mapped.add_vectors(en_embeddings.index_to_key, en_embeddings_mapped)"
      ],
      "metadata": {
        "id": "PSMzrMli9gnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pseudo-dictionary using CSLS for nearest neighbors\n",
        "def build_dictionary(src_emb, tgt_emb, src_words, tgt_words, k=1):\n",
        "    similarities = src_emb @ tgt_emb.T\n",
        "    nn_indices = np.argpartition(-similarities, range(k), axis=1)[:, :k]\n",
        "    word_pairs = []\n",
        "    for i, indices in enumerate(nn_indices):\n",
        "        src_word = src_words[i]\n",
        "        for idx in indices:\n",
        "            tgt_word = tgt_words[idx]\n",
        "            word_pairs.append((src_word, tgt_word))\n",
        "    return word_pairs\n",
        "\n",
        "pseudo_dict = build_dictionary(en_embeddings_mapped, hi_embeddings.vectors, en_embeddings.index_to_key, hi_embeddings.index_to_key, k=1)\n",
        "print(f'Pseudo-dictionary built with {len(pseudo_dict)} word pairs.')\n"
      ],
      "metadata": {
        "id": "YMZMEl4J9t0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding matrices using pseudo-dictionary\n",
        "def create_embedding_matrices(bilingual_dict, source_embeddings, target_embeddings):\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "    oov_count = 0\n",
        "    for src_word, tgt_word in bilingual_dict:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            source_matrix.append(source_embeddings[src_word])\n",
        "            target_matrix.append(target_embeddings[tgt_word])\n",
        "        else:\n",
        "            oov_count += 1\n",
        "    print(f'OOV pairs: {oov_count}')\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "02RPAQBP9yOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_embedding_matrices(pseudo_dict, en_embeddings, hi_embeddings)"
      ],
      "metadata": {
        "id": "UhNl355p-C-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute refined mapping using Procrustes\n",
        "def compute_procrustes(X, Y):\n",
        "    # Compute covariance matrix\n",
        "    M = Y.T @ X\n",
        "    # Singular Value Decomposition\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "    # Compute orthogonal matrix W\n",
        "    W = U @ Vt\n",
        "    return W\n",
        "\n",
        "W_refined = compute_procrustes(X_train, Y_train)"
      ],
      "metadata": {
        "id": "7KgunY3R95q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map English embeddings using refined mapping\n",
        "en_embeddings_refined = en_embeddings.vectors @ W_refined.T\n",
        "en_embeddings_refined = en_embeddings_refined / np.linalg.norm(en_embeddings_refined, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "ppNsVzWt-JaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KeyedVectors instance for refined embeddings\n",
        "en_refined = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "en_refined.add_vectors(en_embeddings.index_to_key, en_embeddings_refined)"
      ],
      "metadata": {
        "id": "WNk2xRK3-NCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute CSLS average similarities\n",
        "def compute_csls(src_emb, tgt_emb, k=10, batch_size=1024):\n",
        "    # Initialize arrays to store average similarities\n",
        "    src_avg_sim = np.zeros(src_emb.shape[0])\n",
        "    tgt_avg_sim = np.zeros(tgt_emb.shape[0])\n",
        "\n",
        "    # Compute source to target similarities in batches\n",
        "    for i in range(0, src_emb.shape[0], batch_size):\n",
        "        src_batch = src_emb[i:i+batch_size]\n",
        "        sims = src_batch @ tgt_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        src_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    # Compute target to source similarities in batches\n",
        "    for i in range(0, tgt_emb.shape[0], batch_size):\n",
        "        tgt_batch = tgt_emb[i:i+batch_size]\n",
        "        sims = tgt_batch @ src_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        tgt_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    return src_avg_sim, tgt_avg_sim\n",
        "\n",
        "src_avg_sim, tgt_avg_sim = compute_csls(en_embeddings_refined, hi_embeddings.vectors, k=K_CSLS, batch_size=1024)"
      ],
      "metadata": {
        "id": "xOlAcEEd-TEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using CSLS\n",
        "def precision_at_k_csls(source_embeddings, target_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    src_vectors = source_embeddings.vectors\n",
        "    tgt_vectors = target_embeddings.vectors\n",
        "\n",
        "    # Build index mappings\n",
        "    src_word2idx = source_embeddings.key_to_index\n",
        "    tgt_idx2word = target_embeddings.index_to_key\n",
        "\n",
        "    # Prepare test data\n",
        "    test_src_indices = []\n",
        "    test_tgt_words = []\n",
        "    for src_word, tgt_word in test_dict:\n",
        "        if src_word in src_word2idx:\n",
        "            test_src_indices.append(src_word2idx[src_word])\n",
        "            test_tgt_words.append(tgt_word)\n",
        "\n",
        "    num_batches = (len(test_src_indices) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_start = i * batch_size\n",
        "        batch_end = min((i + 1) * batch_size, len(test_src_indices))\n",
        "        src_indices_batch = test_src_indices[batch_start:batch_end]\n",
        "        src_vecs = src_vectors[src_indices_batch]\n",
        "        src_csls_sim = src_avg_sim[src_indices_batch]\n",
        "\n",
        "        # Compute CSLS similarities\n",
        "        sims = src_vecs @ tgt_vectors.T\n",
        "        csls_sims = 2 * sims - src_csls_sim[:, None] - tgt_avg_sim[None, :]\n",
        "\n",
        "        # For each source word in the batch\n",
        "        for j in range(csls_sims.shape[0]):\n",
        "            sims_row = csls_sims[j]\n",
        "            top_k_indices = np.argpartition(-sims_row, range(k))[:k]\n",
        "            top_k_words = [tgt_idx2word[idx] for idx in top_k_indices]\n",
        "            tgt_word = test_tgt_words[batch_start + j]\n",
        "            if tgt_word in top_k_words:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    precision = correct / total if total > 0 else 0\n",
        "    return precision\n",
        "\n",
        "# Evaluate\n",
        "p_at_1_csls = precision_at_k_csls(en_refined, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024)\n",
        "p_at_5_csls = precision_at_k_csls(en_refined, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=5, batch_size=1024)\n",
        "\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@1: {p_at_1_csls:.4f}')\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@5: {p_at_5_csls:.4f}')\n",
        "\n",
        "# Plot Precision Scores\n",
        "sizes = ['Unsupervised']\n",
        "p1_scores = [p_at_1_csls]\n",
        "p5_scores = [p_at_5_csls]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(sizes, p1_scores, label='Precision@1')\n",
        "plt.bar(sizes, p5_scores, bottom=p1_scores, label='Precision@5')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Unsupervised Alignment Precision')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uvCgglM8-XKQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}