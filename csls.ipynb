{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekWPK94cZ2NY",
        "outputId": "b6b683e9-6407-4bff-f328-ef36e728b4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==4.3.3\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UliMxVAaufL",
        "outputId": "dad4f331-ed8b-4a8f-810c-6e04d6a55a54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 13:38:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.144.87, 18.154.144.102, 18.154.144.74, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.154.144.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-09-24 13:38:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.144.87, 18.154.144.102, 18.154.144.74, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.154.144.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# English embeddings\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz', binary=False)\n",
        "\n",
        "# Hindi embeddings\n",
        "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz', binary=False)"
      ],
      "metadata": {
        "id": "_UTNFytIa6MB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deb8rvhQelWb",
        "outputId": "5c3113a5-d120-4200-eef1-b2fe5cd3ec2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 13:32:20--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.25, 13.226.210.78, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 219327 (214K) [text/x-c++]\n",
            "Saving to: ‘en-hi.0-5000.txt’\n",
            "\n",
            "\ren-hi.0-5000.txt      0%[                    ]       0  --.-KB/s               \ren-hi.0-5000.txt    100%[===================>] 214.19K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-09-24 13:32:20 (7.82 MB/s) - ‘en-hi.0-5000.txt’ saved [219327/219327]\n",
            "\n",
            "--2024-09-24 13:32:20--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.25, 13.226.210.78, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52464 (51K) [text/plain]\n",
            "Saving to: ‘en-hi.5000-6500.txt’\n",
            "\n",
            "en-hi.5000-6500.txt 100%[===================>]  51.23K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-09-24 13:32:20 (3.64 MB/s) - ‘en-hi.5000-6500.txt’ saved [52464/52464]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# For English\n",
        "en_embeddings.vectors = en_embeddings.vectors[:100000]\n",
        "en_embeddings.index_to_key = en_embeddings.index_to_key[:100000]\n",
        "en_embeddings.key_to_index = {word: idx for idx, word in enumerate(en_embeddings.index_to_key)}\n",
        "\n",
        "# For Hindi\n",
        "hi_embeddings.vectors = hi_embeddings.vectors[:100000]\n",
        "hi_embeddings.index_to_key = hi_embeddings.index_to_key[:100000]\n",
        "hi_embeddings.key_to_index = {word: idx for idx, word in enumerate(hi_embeddings.index_to_key)}"
      ],
      "metadata": {
        "id": "Ma6NxftUep2B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import KeyedVectors\n",
        "import gc"
      ],
      "metadata": {
        "id": "As83UuosuZ8y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 100000  # Adjust based on available memory\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 64  # Adjust based on memory constraints\n",
        "N_EPOCHS = 10  # Number of training epochs\n",
        "K_CSLS = 10  # Number of nearest neighbors for CSLS"
      ],
      "metadata": {
        "id": "Ns8kOA98vPwr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze_AvTu9vUQ6",
        "outputId": "e7248450-d862-407a-b026-57c99d6d9e65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 14:24:51--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.78, 13.226.210.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-09-24 14:24:51--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.78, 13.226.210.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the bilingual lexicon\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        bilingual_dict = [line.strip().split() for line in f]\n",
        "    return bilingual_dict\n",
        "\n",
        "# Loading training dictionary\n",
        "train_dict = load_bilingual_lexicon('en-hi.0-5000.txt')"
      ],
      "metadata": {
        "id": "Gu0DnXVBvWzq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict = load_bilingual_lexicon('en-hi.0-5000.txt')\n",
        "test_dict = load_bilingual_lexicon('en-hi.5000-6500.txt')"
      ],
      "metadata": {
        "id": "2HQkQdyvvayj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEGNki39vkDq",
        "outputId": "46caa634-5a65-4a9d-89e7-5c2bc556c63b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 14:25:03--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.78, 13.226.210.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-09-24 14:25:03--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.78, 13.226.210.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n"
      ],
      "metadata": {
        "id": "vPq4ATw5URov"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz', binary=False)\n",
        "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz', binary=False)\n"
      ],
      "metadata": {
        "id": "XEFYecWUUW8G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_embeddings.vectors = en_embeddings.vectors[:MAX_VOCAB_SIZE]\n",
        "en_embeddings.index_to_key = en_embeddings.index_to_key[:MAX_VOCAB_SIZE]\n",
        "en_embeddings.key_to_index = {word: idx for idx, word in enumerate(en_embeddings.index_to_key)}\n",
        "\n",
        "hi_embeddings.vectors = hi_embeddings.vectors[:MAX_VOCAB_SIZE]\n",
        "hi_embeddings.index_to_key = hi_embeddings.index_to_key[:MAX_VOCAB_SIZE]\n",
        "hi_embeddings.key_to_index = {word: idx for idx, word in enumerate(hi_embeddings.index_to_key)}\n",
        "\n",
        "# Step 5: Prepare Embeddings for Training\n",
        "# Convert embeddings to torch tensors\n",
        "src_embeddings = torch.from_numpy(en_embeddings.vectors).float()\n",
        "tgt_embeddings = torch.from_numpy(hi_embeddings.vectors).float()\n",
        "\n",
        "# Normalize embeddings\n",
        "src_embeddings = src_embeddings / src_embeddings.norm(dim=1, keepdim=True)\n",
        "tgt_embeddings = tgt_embeddings / tgt_embeddings.norm(dim=1, keepdim=True)\n",
        "\n",
        "# Move embeddings to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "src_embeddings = src_embeddings.to(device)\n",
        "tgt_embeddings = tgt_embeddings.to(device)\n"
      ],
      "metadata": {
        "id": "uYbUY0TQjM7n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1So8kqXjPtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=2048):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "Ugwr5DTPw9tC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_93wes5erwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize mapping and discriminator\n",
        "mapping = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n",
        "nn.init.eye_(mapping.weight)  # Initialize as identity matrix\n",
        "discriminator = Discriminator(EMBEDDING_DIM)\n",
        "\n",
        "# Move models to device\n",
        "mapping = mapping.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "# Step 7: Set Up Optimizers\n",
        "mapping_optimizer = optim.SGD(mapping.parameters(), lr=0.1)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "\n",
        "# Step 8: Adversarial Training Loop\n",
        "num_embeddings = src_embeddings.size(0)\n",
        "for epoch in range(N_EPOCHS):\n",
        "    # Shuffle indices\n",
        "    indices = torch.randperm(num_embeddings)\n",
        "    num_batches = num_embeddings // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Get batch indices\n",
        "        batch_indices = indices[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        # Get source and target batch embeddings\n",
        "        src_batch = src_embeddings[batch_indices]\n",
        "        tgt_batch = tgt_embeddings[batch_indices]\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch)\n",
        "        # Discriminator outputs\n",
        "        src_preds = discriminator(src_mapped.detach())\n",
        "        tgt_preds = discriminator(tgt_batch)\n",
        "        # Labels\n",
        "        src_labels = torch.zeros(src_batch.size(0), 1).to(device)\n",
        "        tgt_labels = torch.ones(tgt_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        d_loss_src = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        d_loss_tgt = nn.functional.binary_cross_entropy_with_logits(tgt_preds, tgt_labels)\n",
        "        d_loss = d_loss_src + d_loss_tgt\n",
        "        # Backpropagation\n",
        "        d_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train mapping (generator)\n",
        "        mapping_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch)\n",
        "        # Discriminator output\n",
        "        src_preds = discriminator(src_mapped)\n",
        "        # Labels (want discriminator to think mapped embeddings are target)\n",
        "        src_labels = torch.ones(src_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        g_loss = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        # Backpropagation\n",
        "        g_loss.backward()\n",
        "        mapping_optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyi1BbQTjkfI",
        "outputId": "73b6fa81-c7dc-46ba-ccb2-ba2159739fe1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Discriminator Loss: 1.0607, Generator Loss: 1.6342\n",
            "Epoch 2/10, Discriminator Loss: 0.9434, Generator Loss: 1.8573\n",
            "Epoch 3/10, Discriminator Loss: 0.9147, Generator Loss: 1.7952\n",
            "Epoch 4/10, Discriminator Loss: 0.9492, Generator Loss: 1.7618\n",
            "Epoch 5/10, Discriminator Loss: 1.3018, Generator Loss: 2.1251\n",
            "Epoch 6/10, Discriminator Loss: 1.0209, Generator Loss: 2.1135\n",
            "Epoch 7/10, Discriminator Loss: 0.8881, Generator Loss: 2.3307\n",
            "Epoch 8/10, Discriminator Loss: 0.8039, Generator Loss: 2.3603\n",
            "Epoch 9/10, Discriminator Loss: 0.7237, Generator Loss: 3.0407\n",
            "Epoch 10/10, Discriminator Loss: 0.7870, Generator Loss: 3.1606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_adv = mapping.weight.data.cpu().numpy()\n",
        "\n",
        "# Map source embeddings\n",
        "en_mapped_embeddings = src_embeddings.cpu().numpy() @ W_adv.T"
      ],
      "metadata": {
        "id": "MEsrn9HDOp_k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_mapped_embeddings = src_embeddings.cpu().numpy() @ W_adv.T\n",
        "\n",
        "# Normalize mapped embeddings\n",
        "en_mapped_embeddings = en_mapped_embeddings / np.linalg.norm(en_mapped_embeddings, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "CQMlqvU6rahM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dictionary(src_emb, tgt_emb, src_words, tgt_words, k=1):\n",
        "    src_emb_norm = src_emb / np.linalg.norm(src_emb, axis=1, keepdims=True)\n",
        "    tgt_emb_norm = tgt_emb / np.linalg.norm(tgt_emb, axis=1, keepdims=True)\n",
        "    similarities = src_emb_norm @ tgt_emb_norm.T\n",
        "    nn_indices = np.argpartition(-similarities, range(k), axis=1)[:, :k]\n",
        "    word_pairs = []\n",
        "    for i, indices in enumerate(nn_indices):\n",
        "        src_word = src_words[i]\n",
        "        for idx in indices:\n",
        "            tgt_word = tgt_words[idx]\n",
        "            word_pairs.append((src_word, tgt_word))\n",
        "    return word_pairs\n",
        "\n",
        "# Build pseudo-dictionary\n",
        "pseudo_dict = build_dictionary(en_mapped_embeddings, hi_embeddings.vectors, en_embeddings.index_to_key, hi_embeddings.index_to_key, k=1)\n",
        "\n",
        "# Step 11: Refine the Mapping using Procrustes\n",
        "def create_embedding_matrices(bilingual_dict, source_embeddings, target_embeddings):\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "    for src_word, tgt_word in bilingual_dict:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            source_matrix.append(source_embeddings[src_word])\n",
        "            target_matrix.append(target_embeddings[tgt_word])\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "# Create matrices using pseudo-dictionary\n",
        "X_train, Y_train = create_embedding_matrices(pseudo_dict, en_embeddings, hi_embeddings)\n",
        "\n",
        "# Compute refined mapping\n",
        "def compute_procrustes(X, Y):\n",
        "    # Center the embeddings\n",
        "    X_mean = X.mean(0)\n",
        "    Y_mean = Y.mean(0)\n",
        "    X -= X_mean\n",
        "    Y -= Y_mean\n",
        "    # Compute covariance matrix\n",
        "    M = Y.T @ X\n",
        "    # Singular Value Decomposition\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "    # Compute orthogonal matrix W\n",
        "    W = U @ Vt\n",
        "    return W\n",
        "\n",
        "W_refined = compute_procrustes(X_train, Y_train)\n",
        "\n",
        "# Map English embeddings using refined mapping\n",
        "en_mapped_embeddings = (en_embeddings.vectors - en_embeddings.vectors.mean(0)) @ W_refined.T + hi_embeddings.vectors.mean(0)\n",
        "\n",
        "# Normalize mapped embeddings\n",
        "en_mapped_embeddings = en_mapped_embeddings / np.linalg.norm(en_mapped_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Create KeyedVectors instance\n",
        "en_mapped = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "en_mapped.add_vectors(en_embeddings.index_to_key, en_mapped_embeddings)\n",
        "\n",
        "# Step 12: Compute CSLS Similarities\n",
        "def compute_csls(src_emb, tgt_emb, k=10, batch_size=1024):\n",
        "    src_avg_sim = np.zeros(src_emb.shape[0])\n",
        "    tgt_avg_sim = np.zeros(tgt_emb.shape[0])\n",
        "\n",
        "    # Compute source to target similarities in batches\n",
        "    for i in range(0, src_emb.shape[0], batch_size):\n",
        "        src_batch = src_emb[i:i+batch_size]\n",
        "        sims = src_batch @ tgt_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        src_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    # Compute target to source similarities in batches\n",
        "    for i in range(0, tgt_emb.shape[0], batch_size):\n",
        "        tgt_batch = tgt_emb[i:i+batch_size]\n",
        "        sims = tgt_batch @ src_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        tgt_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    return src_avg_sim, tgt_avg_sim\n",
        "\n",
        "# Compute CSLS average similarities\n",
        "src_avg_sim, tgt_avg_sim = compute_csls(en_mapped_embeddings, hi_embeddings.vectors, k=K_CSLS, batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tQjItMRFriZ8",
        "outputId": "180d01d9-7a74-4f96-aa6d-7b13f1a61166"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'en_mapped_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ba36cc94adbc>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Build pseudo-dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpseudo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_mapped_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Step 11: Refine the Mapping using Procrustes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'en_mapped_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k_csls(source_embeddings, target_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    src_vectors = source_embeddings.vectors\n",
        "    tgt_vectors = target_embeddings.vectors\n",
        "\n",
        "    # Build index mappings\n",
        "    src_word2idx = source_embeddings.key_to_index\n",
        "    tgt_word2idx = target_embeddings.key_to_index\n",
        "\n",
        "    # Prepare test data\n",
        "    test_src_indices = []\n",
        "    test_tgt_indices = []\n",
        "    for src_word, tgt_word in test_dict:\n",
        "        if src_word in src_word2idx and tgt_word in tgt_word2idx:\n",
        "            test_src_indices.append(src_word2idx[src_word])\n",
        "            test_tgt_indices.append(tgt_word2idx[tgt_word])\n",
        "\n",
        "    num_batches = (len(test_src_indices) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_start = i * batch_size\n",
        "        batch_end = min((i + 1) * batch_size, len(test_src_indices))\n",
        "        src_indices_batch = test_src_indices[batch_start:batch_end]\n",
        "        tgt_indices_batch = test_tgt_indices[batch_start:batch_end]\n",
        "        src_vecs = src_vectors[src_indices_batch]\n",
        "        src_csls_sim = src_avg_sim[src_indices_batch]\n",
        "\n",
        "        # Compute CSLS similarities\n",
        "        sims = src_vecs @ tgt_vectors.T\n",
        "        csls_sims = 2 * sims - src_csls_sim[:, None] - tgt_avg_sim[None, :]\n",
        "\n",
        "        # For each source word in the batch\n",
        "        for j in range(csls_sims.shape[0]):\n",
        "            sims_row = csls_sims[j]\n",
        "            top_k_indices = np.argpartition(-sims_row, range(k))[:k]\n",
        "            if tgt_indices_batch[j] in top_k_indices:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    precision = correct / total if total > 0 else 0\n",
        "    return precision\n",
        "\n",
        "# Evaluate\n",
        "p_at_1_csls = precision_at_k_csls(en_mapped, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024)\n",
        "p_at_5_csls = precision_at_k_csls(en_mapped, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=5, batch_size=1024)\n",
        "\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@1: {p_at_1_csls:.4f}')\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@5: {p_at_5_csls:.4f}')"
      ],
      "metadata": {
        "id": "kMk47BDtsHGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}