{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2dI0DwFJ2HXX7edIsmXKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeDieYoung27/Sarvam/blob/main/csls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekWPK94cZ2NY",
        "outputId": "bb23dc00-8df8-4a75-fbe8-2538f892d181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.3) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==4.3.3\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UliMxVAaufL",
        "outputId": "75ffe40b-3d69-4baf-ae70-fe6c0cf9ae2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 15:54:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.78, 13.226.210.25, 13.226.210.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-09-24 15:54:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.78, 13.226.210.25, 13.226.210.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 10000  # Adjust based on available memory\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 64  # Adjust based on memory constraints\n",
        "N_EPOCHS = 5  # Number of training epochs\n",
        "K_CSLS = 10  # Number of nearest neighbors for CSLS\n",
        "K_PRECISION = [1, 5]  # Precision@1 and Precision@5"
      ],
      "metadata": {
        "id": "Yej-bQITv9Gj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHpOqCd7wAU2",
        "outputId": "5bfcdd85-c6f3-412b-8cac-bc38331a4caf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-24 17:28:26--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.78, 13.226.210.25, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-09-24 17:28:27--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.78, 13.226.210.25, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bilingual_lexicon(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                bilingual_dict.append((parts[0].lower(), parts[1].lower()))\n",
        "    return bilingual_dict"
      ],
      "metadata": {
        "id": "tvY0sLrOwFET"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict = load_bilingual_lexicon('en-hi.0-5000.txt')\n",
        "test_dict = load_bilingual_lexicon('en-hi.5000-6500.txt')"
      ],
      "metadata": {
        "id": "tmk0jFMFwIkT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip -k cc.en.300.vec.gz\n",
        "!gunzip -k cc.hi.300.vec.gz"
      ],
      "metadata": {
        "id": "YquwFLKswRPS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_relevant_embeddings(embedding_file, relevant_words, max_vocab_size):\n",
        "    embeddings = {}\n",
        "    with open(embedding_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        next(f)  # Skip header\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(' ')\n",
        "            word = parts[0].lower()\n",
        "            if word in relevant_words:\n",
        "                vector = np.array(parts[1:], dtype=np.float32)\n",
        "                embeddings[word] = vector\n",
        "                if len(embeddings) >= max_vocab_size:\n",
        "                    break\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "fuJnMSioxPWm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_unique_words(bilingual_dicts):\n",
        "    src_words = set()\n",
        "    tgt_words = set()\n",
        "    for bilingual_dict in bilingual_dicts:\n",
        "        for src_word, tgt_word in bilingual_dict:\n",
        "            src_words.add(src_word)\n",
        "            tgt_words.add(tgt_word)\n",
        "    return src_words, tgt_words"
      ],
      "metadata": {
        "id": "nmHdhzw0xVHL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_words, tgt_words = collect_unique_words([train_dict, test_dict])\n",
        "\n",
        "# Load embeddings\n",
        "en_embeddings_dict = load_relevant_embeddings('cc.en.300.vec', src_words, MAX_VOCAB_SIZE)\n",
        "hi_embeddings_dict = load_relevant_embeddings('cc.hi.300.vec', tgt_words, MAX_VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "kkmt9UR6xYqL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KeyedVectors\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import KeyedVectors\n",
        "import gc\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def create_keyed_vectors(embeddings_dict):\n",
        "    kv = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "    kv.add_vectors(list(embeddings_dict.keys()), list(embeddings_dict.values()))\n",
        "    return kv\n",
        "\n",
        "en_embeddings = create_keyed_vectors(en_embeddings_dict)\n",
        "hi_embeddings = create_keyed_vectors(hi_embeddings_dict)\n",
        "\n",
        "# Step 5: Prepare Embeddings for Training\n",
        "# Convert embeddings to torch tensors\n",
        "src_embeddings = torch.from_numpy(en_embeddings.vectors).float()\n",
        "tgt_embeddings = torch.from_numpy(hi_embeddings.vectors).float()\n",
        "\n",
        "# Normalize embeddings\n",
        "src_embeddings = src_embeddings / src_embeddings.norm(2, dim=1, keepdim=True)\n",
        "tgt_embeddings = tgt_embeddings / tgt_embeddings.norm(2, dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "m6SdIx2SFNdc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=2048):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "Y1So8kqXjPtn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n",
        "nn.init.eye_(mapping.weight)\n",
        "discriminator = Discriminator(EMBEDDING_DIM)\n",
        "\n",
        "# Move models to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mapping = mapping.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "src_embeddings = src_embeddings.to(device)\n",
        "tgt_embeddings = tgt_embeddings.to(device)\n",
        "\n",
        "# Step 7: Set Up Optimizers\n",
        "lr = 0.1\n",
        "mapping_optimizer = optim.SGD(mapping.parameters(), lr=lr)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "JgO7si0dFpqt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # Shuffle indices\n",
        "    indices = torch.randperm(src_embeddings.size(0))\n",
        "    num_batches = src_embeddings.size(0) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        # Get batch indices\n",
        "        batch_indices = indices[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        # Get source and target batch embeddings\n",
        "        src_batch = src_embeddings[batch_indices]\n",
        "        tgt_batch = tgt_embeddings[batch_indices]\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch)\n",
        "        # Discriminator outputs\n",
        "        src_preds = discriminator(src_mapped.detach())\n",
        "        tgt_preds = discriminator(tgt_batch)\n",
        "        # Labels\n",
        "        src_labels = torch.zeros(src_batch.size(0), 1).to(device)\n",
        "        tgt_labels = torch.ones(tgt_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        d_loss_src = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        d_loss_tgt = nn.functional.binary_cross_entropy_with_logits(tgt_preds, tgt_labels)\n",
        "        d_loss = d_loss_src + d_loss_tgt\n",
        "        # Backpropagation\n",
        "        d_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train mapping (generator)\n",
        "        mapping_optimizer.zero_grad()\n",
        "        # Map source embeddings\n",
        "        src_mapped = mapping(src_batch)\n",
        "        # Discriminator output\n",
        "        src_preds = discriminator(src_mapped)\n",
        "        # Labels (want discriminator to think mapped embeddings are target)\n",
        "        src_labels = torch.ones(src_batch.size(0), 1).to(device)\n",
        "        # Loss\n",
        "        g_loss = nn.functional.binary_cross_entropy_with_logits(src_preds, src_labels)\n",
        "        # Backpropagation\n",
        "        g_loss.backward()\n",
        "        mapping_optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{N_EPOCHS}, Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_93wes5erwX",
        "outputId": "cfc629be-9f9c-42ea-eb05-6af953d49c8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Discriminator Loss: 0.0000, Generator Loss: 11410032.0000\n",
            "Epoch 2/5, Discriminator Loss: 23.6154, Generator Loss: 53442344.0000\n",
            "Epoch 3/5, Discriminator Loss: 0.0000, Generator Loss: 1268622950400.0000\n",
            "Epoch 4/5, Discriminator Loss: 5409.4761, Generator Loss: 2264265654272.0000\n",
            "Epoch 5/5, Discriminator Loss: 0.0000, Generator Loss: 907598364672.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_adv = mapping.weight.data.cpu().numpy()\n",
        "\n",
        "# Map source embeddings\n",
        "en_mapped_embeddings = en_embeddings.vectors @ W_adv.T\n",
        "\n",
        "# Normalize mapped embeddings\n",
        "en_mapped_embeddings = en_mapped_embeddings / np.linalg.norm(en_mapped_embeddings, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "GkqnRDbrH1gR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dictionary(src_emb, tgt_emb, src_words, tgt_words, k=1):\n",
        "    similarities = src_emb @ tgt_emb.T\n",
        "    nn_indices = np.argpartition(-similarities, range(k), axis=1)[:, :k]\n",
        "    word_pairs = []\n",
        "    for i, indices in enumerate(nn_indices):\n",
        "        src_word = src_words[i]\n",
        "        for idx in indices:\n",
        "            tgt_word = tgt_words[idx]\n",
        "            word_pairs.append((src_word, tgt_word))\n",
        "    return word_pairs\n",
        "\n",
        "# Build pseudo-dictionary\n",
        "pseudo_dict = build_dictionary(en_mapped_embeddings, hi_embeddings.vectors, en_embeddings.index_to_key, hi_embeddings.index_to_key, k=1)\n",
        "print(f'Pseudo-dictionary built with {len(pseudo_dict)} word pairs.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGmV4CUkL3Io",
        "outputId": "7bf817e4-8810-4d7f-933e-c96a02a8ca3f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo-dictionary built with 6484 word pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrices(bilingual_dict, source_embeddings, target_embeddings):\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "    oov_count = 0\n",
        "    for src_word, tgt_word in bilingual_dict:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            source_matrix.append(source_embeddings[src_word])\n",
        "            target_matrix.append(target_embeddings[tgt_word])\n",
        "        else:\n",
        "            oov_count += 1\n",
        "    print(f'OOV pairs: {oov_count}')\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "# Create matrices using pseudo-dictionary\n",
        "X_train, Y_train = create_embedding_matrices(pseudo_dict, en_embeddings, hi_embeddings)\n",
        "\n",
        "# Compute refined mapping\n",
        "def compute_procrustes(X, Y):\n",
        "    # Center the embeddings\n",
        "    X_mean = X.mean(0)\n",
        "    Y_mean = Y.mean(0)\n",
        "    X -= X_mean\n",
        "    Y -= Y_mean\n",
        "    # Compute covariance matrix\n",
        "    M = Y.T @ X\n",
        "    # Singular Value Decomposition\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "    # Compute orthogonal matrix W\n",
        "    W = U @ Vt\n",
        "    return W\n",
        "\n",
        "W_refined = compute_procrustes(X_train, Y_train)\n",
        "\n",
        "# Map English embeddings using refined mapping\n",
        "en_mapped_embeddings = (en_embeddings.vectors - en_embeddings.vectors.mean(0)) @ W_refined.T + hi_embeddings.vectors.mean(0)\n",
        "\n",
        "# Normalize mapped embeddings\n",
        "en_mapped_embeddings = en_mapped_embeddings / np.linalg.norm(en_mapped_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Create KeyedVectors instance\n",
        "en_mapped = KeyedVectors(vector_size=EMBEDDING_DIM)\n",
        "en_mapped.add_vectors(en_embeddings.index_to_key, en_mapped_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJmp2w2YMA60",
        "outputId": "0c074702-ad2b-47eb-9377-fd707387dc82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV pairs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_csls(src_emb, tgt_emb, k=10, batch_size=1024):\n",
        "    # Initialize arrays to store average similarities\n",
        "    src_avg_sim = np.zeros(src_emb.shape[0])\n",
        "    tgt_avg_sim = np.zeros(tgt_emb.shape[0])\n",
        "\n",
        "    # Compute source to target similarities in batches\n",
        "    for i in range(0, src_emb.shape[0], batch_size):\n",
        "        src_batch = src_emb[i:i+batch_size]\n",
        "        sims = src_batch @ tgt_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        src_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    # Compute target to source similarities in batches\n",
        "    for i in range(0, tgt_emb.shape[0], batch_size):\n",
        "        tgt_batch = tgt_emb[i:i+batch_size]\n",
        "        sims = tgt_batch @ src_emb.T\n",
        "        sorted_sims = np.sort(sims, axis=1)[:, -k:]\n",
        "        tgt_avg_sim[i:i+batch_size] = np.mean(sorted_sims, axis=1)\n",
        "\n",
        "    return src_avg_sim, tgt_avg_sim\n",
        "\n",
        "# Compute CSLS average similarities\n",
        "src_avg_sim, tgt_avg_sim = compute_csls(en_mapped_embeddings, hi_embeddings.vectors, k=K_CSLS, batch_size=1024)"
      ],
      "metadata": {
        "id": "ue4Q8hulMFTc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k_csls(source_embeddings, target_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    src_vectors = source_embeddings.vectors\n",
        "    tgt_vectors = target_embeddings.vectors\n",
        "\n",
        "    # Build index mappings\n",
        "    src_word2idx = source_embeddings.key_to_index\n",
        "    tgt_idx2word = target_embeddings.index_to_key\n",
        "\n",
        "    # Prepare test data\n",
        "    test_src_indices = []\n",
        "    test_tgt_words = []\n",
        "    for src_word, tgt_word in test_dict:\n",
        "        if src_word in src_word2idx:\n",
        "            test_src_indices.append(src_word2idx[src_word])\n",
        "            test_tgt_words.append(tgt_word)\n",
        "\n",
        "    num_batches = (len(test_src_indices) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_start = i * batch_size\n",
        "        batch_end = min((i + 1) * batch_size, len(test_src_indices))\n",
        "        src_indices_batch = test_src_indices[batch_start:batch_end]\n",
        "        src_vecs = src_vectors[src_indices_batch]\n",
        "        src_csls_sim = src_avg_sim[src_indices_batch]\n",
        "\n",
        "        # Compute CSLS similarities\n",
        "        sims = src_vecs @ tgt_vectors.T\n",
        "        csls_sims = 2 * sims - src_csls_sim[:, None] - tgt_avg_sim[None, :]\n",
        "\n",
        "        # For each source word in the batch\n",
        "        for j in range(csls_sims.shape[0]):\n",
        "            sims_row = csls_sims[j]\n",
        "            top_k_indices = np.argpartition(-sims_row, range(k))[:k]\n",
        "            top_k_words = [tgt_idx2word[idx] for idx in top_k_indices]\n",
        "            tgt_word = test_tgt_words[batch_start + j]\n",
        "            if tgt_word in top_k_words:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    precision = correct / total if total > 0 else 0\n",
        "    return precision\n",
        "\n",
        "# Evaluate\n",
        "p_at_1_csls = precision_at_k_csls(en_mapped, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=1, batch_size=1024)\n",
        "p_at_5_csls = precision_at_k_csls(en_mapped, hi_embeddings, test_dict, src_avg_sim, tgt_avg_sim, k=5, batch_size=1024)\n",
        "\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@1: {p_at_1_csls:.4f}')\n",
        "print(f'Unsupervised Alignment with CSLS - Precision@5: {p_at_5_csls:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm5VcgK3Nnu7",
        "outputId": "f76a386b-a62f-4fc1-8a75-a396cfa9740e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsupervised Alignment with CSLS - Precision@1: 0.0000\n",
            "Unsupervised Alignment with CSLS - Precision@5: 0.0000\n"
          ]
        }
      ]
    }
  ]
}