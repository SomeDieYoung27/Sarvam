{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZgZ0gfQXeIonnqbt9vVij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeDieYoung27/Sarvam/blob/main/embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSGU8sg5Bq1k"
      },
      "outputs": [],
      "source": [
        "#Loading\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fasttext embedding files\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6v1ad8ftCI20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English embeddings\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz', binary=False)\n",
        "\n",
        "# Hindi embeddings\n",
        "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz', binary=False)"
      ],
      "metadata": {
        "id": "e43fvb7pCOQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#English embeddings\n",
        "en_embeddings.vectors = en_embeddings.vectors[:100000]\n",
        "en_embeddings.index_to_key = en_embeddings.index_to_key[:100000]\n",
        "en_embeddings.key_to_index = {word: idx for idx, word in enumerate(en_embeddings.index_to_key)}\n",
        "\n",
        "#Hindi embeddings\n",
        "hi_embeddings.vectors = hi_embeddings.vectors[:100000]\n",
        "hi_embeddings.index_to_key = hi_embeddings.index_to_key[:100000]\n",
        "hi_embeddings.key_to_index = {word: idx for idx, word in enumerate(hi_embeddings.index_to_key)}"
      ],
      "metadata": {
        "id": "svwPYcXEFuOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bilingual dictionaries download\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.0-5000.txt\n",
        "\n",
        "!wget -c https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AQOv5lvKF8Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating embedding matrices\n",
        "\n",
        "def create_embedding_matrices(bilingual_dict, source_embeddings, target_embeddings):\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "    oov_count = 0\n",
        "    for src_word, tgt_word in bilingual_dict:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            source_matrix.append(source_embeddings[src_word])\n",
        "            target_matrix.append(target_embeddings[tgt_word])\n",
        "        else:\n",
        "            oov_count += 1\n",
        "    print(f'OOV pairs: {oov_count}')\n",
        "    source_matrix = torch.tensor(source_matrix, device=device)\n",
        "    target_matrix = torch.tensor(target_matrix, device=device)\n",
        "    return source_matrix, target_matrix"
      ],
      "metadata": {
        "id": "IgHUMncIF-jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the bilingual lexicon\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        bilingual_dict = [line.strip().split() for line in f]\n",
        "    return bilingual_dict\n"
      ],
      "metadata": {
        "id": "WsheduyjGQPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading training dictionary\n",
        "train_dict = load_bilingual_lexicon('en-hi.0-5000.txt')"
      ],
      "metadata": {
        "id": "WwKXRoB0o0GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading test dictionary\n",
        "test_dict = load_bilingual_lexicon('en-hi.5000-6500.txt')"
      ],
      "metadata": {
        "id": "TsgBQS42GUwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Source and target matrices using torch\n",
        "def create_embedding_matrices(bilingual_dict, source_embeddings, target_embeddings):\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "    oov_count = 0\n",
        "    for src_word, tgt_word in bilingual_dict:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            source_matrix.append(source_embeddings[src_word])\n",
        "            target_matrix.append(target_embeddings[tgt_word])\n",
        "        else:\n",
        "            oov_count += 1\n",
        "    print(f'OOV pairs: {oov_count}')\n",
        "    source_matrix = torch.tensor(source_matrix, device=device)\n",
        "    target_matrix = torch.tensor(target_matrix, device=device)\n",
        "    return source_matrix, target_matrix\n",
        "\n",
        "# Creating matrices\n",
        "X_train, Y_train = create_embedding_matrices(train_dict, en_embeddings, hi_embeddings)\n",
        "\n",
        "# Computing the optimal orthogonal mapping W using torch\n",
        "def compute_procrustes(X, Y):\n",
        "    # Covariance matrix\n",
        "    M = Y.T @ X\n",
        "    # Singular Value Decomposition\n",
        "    U, S, Vh = torch.linalg.svd(M)\n",
        "    # Orthogonal matrix W\n",
        "    W = U @ Vh\n",
        "    return W"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l7zUThjcG5jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = compute_procrustes(X_train, Y_train)\n",
        "\n",
        "# Mapping English embeddings to Hindi space\n",
        "def map_embeddings(embeddings, W):\n",
        "    embeddings = torch.tensor(embeddings, device=device)\n",
        "    mapped_embeddings = embeddings @ W.T\n",
        "    return mapped_embeddings.cpu().numpy()\n",
        "\n",
        "# Map the English embeddings\n",
        "en_mapped_embeddings = map_embeddings(en_embeddings.vectors, W)\n",
        "\n",
        "# Create a new KeyedVectors instance for mapped English embeddings\n",
        "en_mapped = KeyedVectors(vector_size=300)\n",
        "en_mapped.add_vectors(en_embeddings.index_to_key, en_mapped_embeddings)\n"
      ],
      "metadata": {
        "id": "JCg_p4JlHCua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(source_embeddings, target_embeddings, test_dict, k=1):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    target_vectors = torch.tensor(target_embeddings.vectors, device=device)\n",
        "    for src_word, tgt_word in test_dict:\n",
        "        if src_word in source_embeddings.key_to_index:\n",
        "            src_vec = torch.tensor(source_embeddings[src_word], device=device)\n",
        "            # Compute cosine similarities\n",
        "            similarities = torch.nn.functional.cosine_similarity(\n",
        "                target_vectors, src_vec.unsqueeze(0), dim=1\n",
        "            )\n",
        "            # Get top k indices\n",
        "            top_k_indices = torch.topk(similarities, k=k).indices.tolist()\n",
        "            top_k_words = [target_embeddings.index_to_key[i] for i in top_k_indices]\n",
        "            if tgt_word in top_k_words:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    precision = correct / total\n",
        "    return precision\n",
        "\n",
        "# Compute P@1 and P@5\n",
        "p_at_1 = precision_at_k(en_mapped, hi_embeddings, test_dict, k=1)\n",
        "p_at_5 = precision_at_k(en_mapped, hi_embeddings, test_dict, k=5)\n",
        "\n",
        "print(f'Precision@1: {p_at_1:.4f}')\n",
        "print(f'Precision@5: {p_at_5:.4f}')"
      ],
      "metadata": {
        "id": "yAwm1JGNHKsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_cosine_similarities(bilingual_dict, source_embeddings, target_embeddings, W, sample_size=100):\n",
        "    import random\n",
        "    sample_pairs = random.sample(bilingual_dict, sample_size)\n",
        "    similarities = []\n",
        "    for src_word, tgt_word in sample_pairs:\n",
        "        if src_word in source_embeddings.key_to_index and tgt_word in target_embeddings.key_to_index:\n",
        "            src_vec = torch.tensor(source_embeddings[src_word], device=device) @ W.T\n",
        "            tgt_vec = torch.tensor(target_embeddings[tgt_word], device=device)\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(\n",
        "                src_vec.unsqueeze(0), tgt_vec.unsqueeze(0)\n",
        "            ).item()\n",
        "            similarities.append((src_word, tgt_word, cos_sim))\n",
        "    # Sort\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "    return similarities\n",
        "\n",
        "# Get similarities\n",
        "similarities = analyze_cosine_similarities(test_dict, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "# Display top 10 most similar pairs\n",
        "for src_word, tgt_word, sim in similarities[:10]:\n",
        "    print(f'{src_word} - {tgt_word}: {sim:.4f}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1CDTTfYqPCD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sizes = [5000, 10000, 20000]  # Adjust based on available data\n",
        "results = []\n",
        "\n",
        "for size in training_sizes:\n",
        "    # Use the first 'size' word pairs from the training dictionary\n",
        "    current_train_dict = train_dict[:size]\n",
        "    # Create embedding matrices\n",
        "    X_train, Y_train = create_embedding_matrices(current_train_dict, en_embeddings, hi_embeddings)\n",
        "    # Compute mapping\n",
        "    W = compute_procrustes(X_train, Y_train)\n",
        "    # Map English embeddings\n",
        "    en_mapped_embeddings = map_embeddings(en_embeddings.vectors, W)\n",
        "    en_mapped = KeyedVectors(vector_size=300)\n",
        "    en_mapped.add_vectors(en_embeddings.index_to_key, en_mapped_embeddings)\n",
        "    # Evaluate\n",
        "    p_at_1 = precision_at_k(en_mapped, hi_embeddings, test_dict, k=1)\n",
        "    p_at_5 = precision_at_k(en_mapped, hi_embeddings, test_dict, k=5)\n",
        "    results.append((size, p_at_1, p_at_5))\n",
        "    print(f'Training Size: {size}, Precision@1: {p_at_1:.4f}, Precision@5: {p_at_5:.4f}')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3eHY35lzPENz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes, p1_scores, p5_scores = zip(*results)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sizes, p1_scores, label='Precision@1')\n",
        "plt.plot(sizes, p5_scores, label='Precision@5')\n",
        "plt.xlabel('Training Dictionary Size')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Ablation Study: Impact of Training Dictionary Size')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DDP5GjI5kSs5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}